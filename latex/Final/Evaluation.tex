\section{Evaluation}

We evaluated the classifiers from each branch separately (and the recommender system in the profile-based branch).

\subsection{Review-Based Branch}
Due to the large size of the entire dataset (over 1.5 million reviews), we systematically divided the dataset in our evaluation. Moreover, because a part-of-speech tagger significantly increases the running time of extracting all of our features, we measured our classifiers' performances when they contained and did not contain the adjective and adverb count features separately. We first randomly chose 100,000 reviews from the full dataset, from which we randomly selected 70,000 to be the training set and 30,000 to be the test set. The features not including the adjective and adverb counts were then extracted from the training set, and then the classifiers were trained on these features and evaluated on the test set. The process of randomly choosing and splitting 100,000 reviews and training and evaluating the classifiers was repeated 9 more times, and the results were averaged over the 10 trials.

We then randomly chose 10,000 reviews from the full dataset, from which we randomly selected 7,000 to be the training set and 3,000 to be the test set. The full set of features was then extracted from this smaller training set, and the classifiers were again trained and evaluated. As in the larger dataset, we performed a total of 10 trials.

\subsection{Profile-Based Branch}

\subsubsection{Classifier Evaluation}

\subsubsection{Recommender System Evaluation}